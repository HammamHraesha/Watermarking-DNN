{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checkpoint path, if not exists\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.mkdir('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27170bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch-sizes and torch device\n",
    "BATCH_SIZE = 64\n",
    "TRIGGER_BATCH_SIZE = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a derived dataset class to use in dataloader\n",
    "class TriggerSet(Dataset):\n",
    "    def __init__(self, trigger_path, transform=None):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        labels_folder = os.path.join(trigger_path, \"labels\")\n",
    "        images_folder = os.path.join(trigger_path, \"images\")\n",
    "        with open(os.path.join(labels_folder, \"trigger_labels.txt\"), \"r\") as file:\n",
    "            for line in file:\n",
    "                label = int(line.strip())\n",
    "                self.labels.append(label)\n",
    "        for i in range(len(self.labels)):\n",
    "            img_name = f\"{i}.jpg\"\n",
    "            img_path = os.path.join(images_folder, img_name)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            if transform:\n",
    "                img = transform(img)\n",
    "            self.images.append(img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b896867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate given model, on a given data inside a dataloder.\n",
    "    Input:\n",
    "        model = pytorch trained model\n",
    "        dataloader = a dataloader object with test data\n",
    "    Output:\n",
    "        an integer, accuracy of the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    model.train()\n",
    "    return (100 * correct) / total\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, path):\n",
    "    \"\"\"\n",
    "    A function to save given model's, and optimizer's state.\n",
    "    Input:\n",
    "        epoch = an integer, trained number of epochs\n",
    "        optimizer = optimizer object with current status, learnin rate etc.\n",
    "        model = pytorch model with current learnable parameters\n",
    "        path = a string indicating the path of checkpoint\n",
    "    Output:\n",
    "        No output, save the to given path.\n",
    "    \"\"\"\n",
    "    checkpoint_name = f\"checkpoint_{epoch}.pt\"\n",
    "    checkpoint_path = os.path.join(path, checkpoint_name)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"\n",
    "    A function to load given model's, and optimizer's state.\n",
    "    Input:\n",
    "        optimizer =  optimizer object to be updated\n",
    "        model =  pytorch model to be updated\n",
    "        path = model path to be loaded\n",
    "    Output:\n",
    "        No output, save the to given path.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((640, 640))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training environment\n",
    "use_trigger = False\n",
    "dataset = \"cifar10\"\n",
    "trigger_path = \"/kaggle/input/trigger/data/trigger_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9afbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46k-4k training-test split from CIFAR10/CIFAR100-training data\n",
    "if dataset == \"cifar10\":\n",
    "    num_classes = 10\n",
    "    full_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainset = torch.utils.data.Subset(full_dataset, range(46000))\n",
    "    testset = torch.utils.data.Subset(full_dataset, range(46000, 50000))\n",
    "elif dataset == \"cifar100\":\n",
    "    num_classes = 100\n",
    "    full_dataset = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    trainset = torch.utils.data.Subset(full_dataset, range(46000))\n",
    "    testset = torch.utils.data.Subset(full_dataset, range(46000, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64010ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate trigger data set\n",
    "trigger_set = TriggerSet(trigger_path, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "trigger_dataloader = DataLoader(trigger_set, batch_size=TRIGGER_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "model = resnet18(num_classes=num_classes).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317c3174",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-27T19:22:27.964909Z",
     "iopub.status.busy": "2023-05-27T19:22:27.964351Z",
     "iopub.status.idle": "2023-05-28T01:32:49.481672Z",
     "shell.execute_reply": "2023-05-28T01:32:49.480245Z"
    },
    "papermill": {
     "duration": 22224.096907,
     "end_time": "2023-05-28T01:32:52.057565",
     "exception": true,
     "start_time": "2023-05-27T19:22:27.960658",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:10<00:00, 15866714.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Epoch [1/30] Loss: 1.4666 Acc.: 29.67%: 100%|██████████| 719/719 [11:28<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\tTrain Acc.: 29.67%\tTest-set Acc.: 38.83%\tTrigger-set Acc.: 10.00%\tAvg Loss: 1.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/30] Loss: 1.2952 Acc.: 53.41%: 100%|██████████| 719/719 [11:22<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30]\tTrain Acc.: 53.41%\tTest-set Acc.: 60.40%\tTrigger-set Acc.: 10.00%\tAvg Loss: 1.2833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/30] Loss: 0.7900 Acc.: 64.84%: 100%|██████████| 719/719 [11:27<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30]\tTrain Acc.: 64.84%\tTest-set Acc.: 61.17%\tTrigger-set Acc.: 2.00%\tAvg Loss: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/30] Loss: 0.6556 Acc.: 71.10%: 100%|██████████| 719/719 [11:22<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30]\tTrain Acc.: 71.10%\tTest-set Acc.: 61.48%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/30] Loss: 0.7069 Acc.: 74.65%: 100%|██████████| 719/719 [11:23<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30]\tTrain Acc.: 74.65%\tTest-set Acc.: 65.15%\tTrigger-set Acc.: 8.00%\tAvg Loss: 0.7295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/30] Loss: 0.6094 Acc.: 76.72%: 100%|██████████| 719/719 [11:26<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30]\tTrain Acc.: 76.72%\tTest-set Acc.: 68.60%\tTrigger-set Acc.: 12.00%\tAvg Loss: 0.6733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/30] Loss: 0.4749 Acc.: 78.22%: 100%|██████████| 719/719 [11:23<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30]\tTrain Acc.: 78.22%\tTest-set Acc.: 73.12%\tTrigger-set Acc.: 10.00%\tAvg Loss: 0.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/30] Loss: 0.5857 Acc.: 79.43%: 100%|██████████| 719/719 [11:27<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30]\tTrain Acc.: 79.43%\tTest-set Acc.: 73.12%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/30] Loss: 0.5794 Acc.: 80.68%: 100%|██████████| 719/719 [11:28<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30]\tTrain Acc.: 80.68%\tTest-set Acc.: 68.58%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/30] Loss: 0.5021 Acc.: 81.15%: 100%|██████████| 719/719 [11:28<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30]\tTrain Acc.: 81.15%\tTest-set Acc.: 60.17%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.5457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/30] Loss: 0.7438 Acc.: 81.86%: 100%|██████████| 719/719 [11:30<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30]\tTrain Acc.: 81.86%\tTest-set Acc.: 75.58%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/30] Loss: 0.6825 Acc.: 82.52%: 100%|██████████| 719/719 [11:30<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30]\tTrain Acc.: 82.52%\tTest-set Acc.: 74.55%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/30] Loss: 0.6422 Acc.: 83.01%: 100%|██████████| 719/719 [11:29<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30]\tTrain Acc.: 83.01%\tTest-set Acc.: 79.00%\tTrigger-set Acc.: 2.00%\tAvg Loss: 0.4942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/30] Loss: 0.6099 Acc.: 83.72%: 100%|██████████| 719/719 [11:34<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30]\tTrain Acc.: 83.72%\tTest-set Acc.: 70.95%\tTrigger-set Acc.: 8.00%\tAvg Loss: 0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/30] Loss: 0.3301 Acc.: 84.07%: 100%|██████████| 719/719 [11:31<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30]\tTrain Acc.: 84.07%\tTest-set Acc.: 73.28%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.4662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/30] Loss: 0.5989 Acc.: 84.09%: 100%|██████████| 719/719 [11:34<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30]\tTrain Acc.: 84.09%\tTest-set Acc.: 68.40%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/30] Loss: 0.5638 Acc.: 84.40%: 100%|██████████| 719/719 [11:32<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30]\tTrain Acc.: 84.40%\tTest-set Acc.: 77.53%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/30] Loss: 0.8361 Acc.: 84.62%: 100%|██████████| 719/719 [11:30<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30]\tTrain Acc.: 84.62%\tTest-set Acc.: 62.15%\tTrigger-set Acc.: 8.00%\tAvg Loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/30] Loss: 0.3385 Acc.: 84.87%: 100%|██████████| 719/719 [11:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30]\tTrain Acc.: 84.87%\tTest-set Acc.: 72.90%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/30] Loss: 0.5081 Acc.: 85.07%: 100%|██████████| 719/719 [11:34<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30]\tTrain Acc.: 85.07%\tTest-set Acc.: 76.33%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/30] Loss: 0.5752 Acc.: 85.51%: 100%|██████████| 719/719 [11:29<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30]\tTrain Acc.: 85.51%\tTest-set Acc.: 70.15%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/30] Loss: 0.4491 Acc.: 85.57%: 100%|██████████| 719/719 [11:32<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30]\tTrain Acc.: 85.57%\tTest-set Acc.: 72.83%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [23/30] Loss: 0.3759 Acc.: 85.71%: 100%|██████████| 719/719 [11:32<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30]\tTrain Acc.: 85.71%\tTest-set Acc.: 70.38%\tTrigger-set Acc.: 8.00%\tAvg Loss: 0.4171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [24/30] Loss: 0.5469 Acc.: 85.69%: 100%|██████████| 719/719 [11:31<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30]\tTrain Acc.: 85.69%\tTest-set Acc.: 68.53%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [25/30] Loss: 0.3621 Acc.: 85.40%: 100%|██████████| 719/719 [11:32<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30]\tTrain Acc.: 85.40%\tTest-set Acc.: 78.88%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.4222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [26/30] Loss: 0.5753 Acc.: 85.68%: 100%|██████████| 719/719 [11:34<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30]\tTrain Acc.: 85.68%\tTest-set Acc.: 69.42%\tTrigger-set Acc.: 14.00%\tAvg Loss: 0.4160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [27/30] Loss: 0.4979 Acc.: 85.90%: 100%|██████████| 719/719 [11:33<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30]\tTrain Acc.: 85.90%\tTest-set Acc.: 75.67%\tTrigger-set Acc.: 8.00%\tAvg Loss: 0.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [28/30] Loss: 0.3664 Acc.: 86.04%: 100%|██████████| 719/719 [11:32<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30]\tTrain Acc.: 86.04%\tTest-set Acc.: 67.78%\tTrigger-set Acc.: 4.00%\tAvg Loss: 0.4068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [29/30] Loss: 0.5250 Acc.: 85.90%: 100%|██████████| 719/719 [11:34<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30]\tTrain Acc.: 85.90%\tTest-set Acc.: 74.12%\tTrigger-set Acc.: 10.00%\tAvg Loss: 0.4089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [30/30] Loss: 0.4561 Acc.: 86.13%: 100%|██████████| 719/719 [11:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30]\tTrain Acc.: 86.13%\tTest-set Acc.: 76.17%\tTrigger-set Acc.: 6.00%\tAvg Loss: 0.4041\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./checkpoint/model.pt does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m         save_checkpoint(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, model, optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoint/model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(epoch, model, optimizer, path)\u001b[0m\n\u001b[1;32m     59\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, checkpoint_name)\n\u001b[1;32m     60\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     64\u001b[0m }\n\u001b[0;32m---> 65\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./checkpoint/model.pt does not exist."
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for batch_idx, (images, labels) in pbar:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        if use_trigger:\n",
    "            trigger_images, trigger_labels = next(iter(trigger_dataloader))\n",
    "            trigger_images = trigger_images.to(DEVICE)\n",
    "            trigger_labels = trigger_labels.to(DEVICE)\n",
    "\n",
    "            images = torch.cat((images, trigger_images), dim=0)\n",
    "            labels = torch.cat((labels, trigger_labels), dim=0)\n",
    "\n",
    "        # Clean optimizer gradients, get the output, get the loss, update gradients, update parameters\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        pbar.set_description(f\"Epoch [{epoch + 1}/30] Loss: {loss.item():.4f} Acc.: {100.0 * correct / total:.2f}%\")\n",
    "\n",
    "    # epoch loss and accuracy\n",
    "    train_acc = 100.0 * correct / total\n",
    "    avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "    # epoch evaluation\n",
    "    test_acc = evaluate(model, test_dataloader)\n",
    "    trigger_acc = evaluate(model, trigger_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/30]\\tTrain Acc.: {train_acc:.2f}%\\tTest-set Acc.: {test_acc:.2f}%\\tTrigger-set Acc.: {trigger_acc:.2f}%\\tAvg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # save model\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, \"./checkpoint\")\n",
    "\n",
    "# final save\n",
    "save_checkpoint(30, model, optimizer, \"./checkpoint/model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22240.39942,
   "end_time": "2023-05-28T01:32:57.824587",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-27T19:22:17.425167",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
