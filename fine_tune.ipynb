{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set device to the cuda GPU \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trigger set dataset class\n","class TriggerSet(Dataset):\n","    def __init__(self, trigger_path, transform=None):\n","        self.images = []\n","        self.labels = []\n","        labels_folder = os.path.join(trigger_path, \"labels\")\n","        images_folder = os.path.join(trigger_path, \"images\")\n","        with open(os.path.join(labels_folder, \"trigger_labels.txt\"), \"r\") as file:\n","            for line in file:\n","                label = int(line.strip())\n","                self.labels.append(label)\n","        for i in range(len(self.labels)):\n","            img_name = f\"{i+1}.jpg\"\n","            img_path = os.path.join(images_folder, img_name)\n","            img = Image.open(img_path).convert(\"RGB\")\n","            if transform:\n","                img = transform(img)\n","            self.images.append(img)\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rtll_fine_tuning(model, optimizer, train_loader):\n","    \"\"\"\n","    A function to fine-tune given model by training last layer.\n","    Input:\n","        model = pytorch model\n","        optimizer = optimizer object\n","        train_loader = dataset loader which we will use during fine-tuning\n","    Output:\n","        model = fine-tuned model\n","    \"\"\"\n","\n","    # freeze except the last layer\n","    for param in model.parameters():\n","        param.requires_grad = False\n","    for param in model.fc.parameters():\n","        param.requires_grad = True\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    \n","    model.train()\n","    for epoch in range(40):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            running_loss += loss.item()\n","        \n","        print(f\"Epoch {epoch+1} loss: {running_loss / len(train_loader)}\")\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# transform\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# dataset\n","cifar10_test = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","trainset = torch.utils.data.Subset(cifar10_test, range(46000,50000))\n","cifar10_test_loader = DataLoader(trainset, batch_size=64, shuffle=False)\n","\n","# trigger-set\n","trigger_set = TriggerSet(trigger_path=\"/kaggle/input/trigger/data/trigger_set\", transform=transform)\n","trigger_loader = DataLoader(trigger_set, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# uploading trained watermarked model\n","model = models.resnet18(pretrained=False)\n","model.fc = nn.Linear(512, num_classes)\n","model.to(DEVICE)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","checkpoint = torch.load('/kaggle/input/checkpoint/checkpoint/checkpoint_30.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fine-tune\n","model = rtll_fine_tuning(model, optimizer, trigger_loader)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T18:13:26.184430Z","iopub.status.busy":"2023-05-29T18:13:26.183922Z","iopub.status.idle":"2023-05-29T18:14:45.855095Z","shell.execute_reply":"2023-05-29T18:14:45.854113Z","shell.execute_reply.started":"2023-05-29T18:13:26.184390Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Epoch 1 loss: 0.5557218790054321\n","Epoch 2 loss: 0.5368276238441467\n","Epoch 3 loss: 0.5015008449554443\n","Epoch 4 loss: 0.45754948258399963\n","Epoch 5 loss: 0.41262176632881165\n","Epoch 6 loss: 0.37289902567863464\n","Epoch 7 loss: 0.3420593738555908\n","Epoch 8 loss: 0.3210931420326233\n","Epoch 9 loss: 0.30905085802078247\n","Epoch 10 loss: 0.3038449287414551\n","Epoch 11 loss: 0.30274444818496704\n","Epoch 12 loss: 0.30288419127464294\n","Epoch 13 loss: 0.30186450481414795\n","Epoch 14 loss: 0.2981565594673157\n","Epoch 15 loss: 0.29119226336479187\n","Epoch 16 loss: 0.2812535762786865\n","Epoch 17 loss: 0.26926398277282715\n","Epoch 18 loss: 0.256491482257843\n","Epoch 19 loss: 0.2441827952861786\n","Epoch 20 loss: 0.233220174908638\n","Epoch 21 loss: 0.22394314408302307\n","Epoch 22 loss: 0.21620574593544006\n","Epoch 23 loss: 0.2095971554517746\n","Epoch 24 loss: 0.20367416739463806\n","Epoch 25 loss: 0.19810545444488525\n","Epoch 26 loss: 0.19271647930145264\n","Epoch 27 loss: 0.18746748566627502\n","Epoch 28 loss: 0.18240106105804443\n","Epoch 29 loss: 0.17758649587631226\n","Epoch 30 loss: 0.17307667434215546\n","Epoch 31 loss: 0.16888462007045746\n","Epoch 32 loss: 0.16498005390167236\n","Epoch 33 loss: 0.16130118072032928\n","Epoch 34 loss: 0.15777581930160522\n","Epoch 35 loss: 0.15434280037879944\n","Epoch 36 loss: 0.15096713602542877\n","Epoch 37 loss: 0.14764511585235596\n","Epoch 38 loss: 0.14439921081066132\n","Epoch 39 loss: 0.14126650989055634\n","Epoch 40 loss: 0.13828518986701965\n","Accuracy on the trigger set: 100.0%\n","Accuracy on the CIFAR-10 test set: 73.1%\n"]}],"source":["# Test\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in trigger_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Accuracy on the trigger set: {(correct / total) * 100}%\")\n","\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in cifar10_test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Accuracy on the CIFAR-10 test set: {(correct / total) * 100}%\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
